{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = ''\n"
      ],
      "metadata": {
        "id": "ikS_iNATbRts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SK7tOqYr-WYs"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "\n",
        "dataset_folder = '/content/gdrive/My Drive/Colab Notebooks/graph_dataset' + model\n",
        "\n",
        "MODEL_FILE   = 'classification_model.bin'\n",
        "MODEL_CONFIG_FILE = 'classification_model_config.json'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AfhoR28cDy1m",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Install torch/pyg\n",
        "\n",
        "!pip install torchmetrics\n",
        "!pip install class_resolver\n",
        "\n",
        "import os\n",
        "import torch\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)\n",
        "\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "b_v5P4GOAYlb"
      },
      "outputs": [],
      "source": [
        "#@title Constants\n",
        "\n",
        "SENTENCE_EMB = 1\n",
        "TOKEN_EMB = 2\n",
        "\n",
        "INTERACTION_CLASSES = {'negative', 'advise', 'effect', 'mechanism', 'int'}\n",
        "TOKEN_CLASS = 'Token'\n",
        "SENT_CLASS = 'Sentence'\n",
        "RDF_TYPE = 'rdf_type'\n",
        "RDF_S = 'rdf_subject'\n",
        "RDF_P = 'rdf_predicate'\n",
        "RDF_O = 'rdf_object'\n",
        "DOMAIN = 'rdfs_domain'\n",
        "RANGE = 'rdfs_range'\n",
        "DRUG_CLASS = 'Drug_Class'\n",
        "SAME_AS = 'owl_sameAs'\n",
        "INTERACTION_CLASS = 'Interaction'\n",
        "\n",
        "SENT_SOURCE = 'source'\n",
        "ENT_FOUND_AS = 'found_as'\n",
        "INT_FOUND = 'interaction_found'\n",
        "SENT_CON_INTER = 'contains_interaction'\n",
        "SENT_CON_TOKEN = 'contains_token'\n",
        "SENT_DOC = 'document'\n",
        "WITH_TOKEN = 'with_token'\n",
        "TOKEN_IN_INT = 'in_interaction'\n",
        "\n",
        "\n",
        "\n",
        "DOMAIN_RANGE = {\n",
        "    ENT_FOUND_AS :      {DOMAIN: DRUG_CLASS,        RANGE: TOKEN_CLASS},\n",
        "    INT_FOUND:          {DOMAIN: DRUG_CLASS,        RANGE: INTERACTION_CLASS},\n",
        "    SAME_AS:            {DOMAIN: DRUG_CLASS,        RANGE: DRUG_CLASS},\n",
        "    SENT_CON_INTER:     {DOMAIN: SENT_CLASS,        RANGE: INTERACTION_CLASS},\n",
        "    SENT_CON_TOKEN:     {DOMAIN: SENT_CLASS,        RANGE: TOKEN_CLASS},\n",
        "\n",
        "    TOKEN_IN_INT:       {DOMAIN: TOKEN_CLASS,       RANGE: INTERACTION_CLASS},\n",
        "    WITH_TOKEN:         {DOMAIN: INTERACTION_CLASS, RANGE: TOKEN_CLASS},\n",
        "    SENT_SOURCE:        {DOMAIN: INTERACTION_CLASS, RANGE: SENT_CLASS}\n",
        "}\n",
        "\n",
        "INVERSE = {\n",
        "    SENT_SOURCE:    SENT_CON_INTER,\n",
        "    SENT_CON_INTER: SENT_SOURCE,\n",
        "\n",
        "    WITH_TOKEN:     TOKEN_IN_INT,\n",
        "    TOKEN_IN_INT:   WITH_TOKEN\n",
        "}\n",
        "\n",
        "\n",
        "def get_domain_range(predicate):\n",
        "    try:\n",
        "        dr = DOMAIN_RANGE[predicate]\n",
        "        return dr[DOMAIN], dr[RANGE]\n",
        "    except KeyError:\n",
        "        return None, None\n",
        "\n",
        "def get_inverse(predicate):\n",
        "    return INVERSE.get(predicate, None)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "PlMwzsXlC86b"
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "\n",
        "\n",
        "import random\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torchmetrics\n",
        "from torchmetrics import ConfusionMatrix\n",
        "from torch.nn.functional import softmax\n",
        "from torch.nn import Dropout\n",
        "from torch.nn.functional import dropout\n",
        "from torch import tanh\n",
        "from torch.nn.functional import leaky_relu, relu\n",
        "\n",
        "\n",
        "import torch_geometric\n",
        "from torch_geometric.utils import dropout_adj\n",
        "from torch_geometric.nn import Linear\n",
        "\n",
        "from torch_geometric.data import HeteroData, Dataset\n",
        "from torch_geometric.loader import HGTLoader\n",
        "from torch_geometric.transforms import RandomNodeSplit\n",
        "\n",
        "from torch_geometric.nn.models import MLP\n",
        "from torch_geometric.nn import SAGEConv, GINConv, GATConv, GCNConv, BatchNorm, HeteroConv\n",
        "\n",
        "\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "import json\n",
        "from os.path import exists\n",
        "\n",
        "\n",
        "\n",
        "pd.set_option('display.width', 500)\n",
        "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
        "\n",
        "\n",
        "\n",
        "def set_seed():\n",
        "    random.seed(0)\n",
        "    np.random.seed(0)\n",
        "    torch.manual_seed(0)\n",
        "    torch.cuda.manual_seed(0)\n",
        "    torch.cuda.manual_seed_all(0)\n",
        "\n",
        "    torch_geometric.seed_everything(0)\n",
        "\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "def device():\n",
        "    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "def graph_dataset(train):\n",
        "    return dataset_folder + ('/train' if train else '/test')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "kyoFx-8FBhoG"
      },
      "outputs": [],
      "source": [
        "#@title Load word embeddings \n",
        "\n",
        "class WordEmbeddings:\n",
        "\n",
        "    def __init__(self, train, sentences_init=None):\n",
        "\n",
        "        self.sentence_emb_file = WordEmbeddings.embeddings_file_path(train, SENTENCE_EMB)\n",
        "        self.token_emb_file    = WordEmbeddings.embeddings_file_path(train, TOKEN_EMB)\n",
        "        self.files_exist       = exists(self.sentence_emb_file) and exists(self.token_emb_file)\n",
        "        if not self.files_exist:\n",
        "            if sentences_init is None:\n",
        "                raise Exception('Files not found and sentence_init arg eq to None')\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def get_embbedings_files(train):\n",
        "        return [WordEmbeddings.embeddings_file_path(train, SENTENCE_EMB),\n",
        "                WordEmbeddings.embeddings_file_path(train, TOKEN_EMB)]\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def embeddings_file_path(train, embeddings_mode):\n",
        "        return graph_dataset(train) + '/raw/' + \\\n",
        "            ('sentence' if embeddings_mode == SENTENCE_EMB else 'token')\\\n",
        "            + '_embeddings.pt'\n",
        "\n",
        "\n",
        "    def word_embeddings(self):\n",
        "\n",
        "        embeddings = {}\n",
        "        for (file, embeddings_mode) in [(self.sentence_emb_file, SENTENCE_EMB), (self.token_emb_file, TOKEN_EMB)]:\n",
        "\n",
        "            embeddings[embeddings_mode] = \\\n",
        "                        torch.load(file) if self.files_exist \\\n",
        "                        else None\n",
        "\n",
        "        return embeddings[SENTENCE_EMB], embeddings[TOKEN_EMB]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gSSqgMTEwgt",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title Graph Dataset\n",
        "\n",
        "\n",
        "ZERO_INIT = 0\n",
        "\n",
        "\n",
        "class GraphDataset(Dataset):\n",
        "\n",
        "    def __init__(self, train, add_inverse=False):\n",
        "        self.train = train\n",
        "        self.f = self.set_files()\n",
        "        self.label_enc = LabelEncoder().fit(list(INTERACTION_CLASSES))\n",
        "        self.negative_label = self.label_enc.transform(['negative'])[0]\n",
        "\n",
        "        super(GraphDataset, self).__init__(root=graph_dataset(self.train), transform=None)\n",
        "\n",
        "        if add_inverse:\n",
        "            self.add_inverse_edges()\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "    def set_files(self):\n",
        "        files = {}\n",
        "        path = graph_dataset(self.train)\n",
        "        files['processed_path'] = path + '/processed'\n",
        "        files['saved_dataset'] = f\"{files['processed_path']}/dataset.pt\"\n",
        "        files['dataset_info'] = f\"{files['processed_path']}/dataset_info.data\"\n",
        "        files['files'] = [files['saved_dataset'], files['dataset_info']]\n",
        "\n",
        "        return files\n",
        "\n",
        "    @property\n",
        "    def raw_file_names(self):\n",
        "        \"\"\"\n",
        "        Τα αρχεία που πρέπει να είναι αποθηκευμένα ώστε να μη χρειαστεί να τα φτιάξει\n",
        "        \"\"\"\n",
        "        return self.f['files']\n",
        "\n",
        "    def download(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "    def save_dataset(self):\n",
        "        if not exists(self.f['processed_path']):\n",
        "            mkdir(self.f['processed_path'])\n",
        "        torch.save(self.graph, self.f['saved_dataset'])\n",
        "        dataset_info = {'n_interaction_nodes': self.n_interaction_nodes,\n",
        "                        'node_indexes': self.node_indexes,\n",
        "                        'emb_dim': self.emb_dim}\n",
        "        with open(self.f['dataset_info'], 'wb') as filehandle:\n",
        "            pickle.dump(dataset_info, filehandle)\n",
        "\n",
        "    @property\n",
        "    def processed_file_names(self):\n",
        "        return 'run_process'\n",
        "\n",
        "    def process(self):\n",
        "        self.graph = torch.load(self.f['saved_dataset'])\n",
        "        dataset_info = pickle.load(open(self.f['dataset_info'], 'rb'))\n",
        "        self.n_interaction_nodes = dataset_info['n_interaction_nodes']\n",
        "        self.node_indexes = dataset_info['node_indexes']\n",
        "        self.emb_dim = dataset_info['emb_dim']\n",
        "\n",
        "\n",
        "    # ---------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "    def create_graph(self, subgraph, node_labels, nodes_per_class):\n",
        "        node_indexes, node_feature_vectors, edge_index_matrices, node_labels, emb_dim = \\\n",
        "            self.create_vectors(subgraph, node_labels, nodes_per_class)\n",
        "\n",
        "        graph = HeteroData()\n",
        "        for node_type, feature_vector in node_feature_vectors.items():\n",
        "            graph[node_type].x = feature_vector\n",
        "\n",
        "        graph[INTERACTION_CLASS].y = node_labels\n",
        "        n_interaction_nodes = graph[INTERACTION_CLASS].y.size(0)\n",
        "\n",
        "        self.initialize_interaction_nodes_feature_vec(graph=graph, emb_dim=emb_dim)\n",
        "\n",
        "        graph[INTERACTION_CLASS].train_mask = torch.ones(n_interaction_nodes, dtype=torch.bool)\n",
        "\n",
        "        for edge_type, edge_index in edge_index_matrices.items():\n",
        "            graph[edge_type].edge_index = edge_index\n",
        "\n",
        "        return graph, n_interaction_nodes, node_indexes, emb_dim\n",
        "\n",
        "\n",
        "\n",
        "    def initialize_interaction_nodes_feature_vec(self, graph=None, emb_dim=None, method=ZERO_INIT):\n",
        "        if graph is None: graph = self.graph\n",
        "        if emb_dim is None: emb_dim = self.emb_dim\n",
        "\n",
        "        n_nodes = graph[INTERACTION_CLASS].y.size(0)\n",
        "        if method == ZERO_INIT:\n",
        "            feauture_vector = torch.zeros(size=(n_nodes, emb_dim), dtype=torch.float32)\n",
        "        else:\n",
        "            raise Exception('Invalid init method')\n",
        "\n",
        "        graph[INTERACTION_CLASS].x = feauture_vector\n",
        "\n",
        "\n",
        "\n",
        "    def create_vectors(self, subgraph, node_labels, nodes_per_class):\n",
        "        \"\"\"\n",
        "        :param subgraph : DF<RDF_S, RDF_P, RDF_O> με τα κατηγορήματα (ακμές που θα μπουν στο γράφο)\n",
        "        :param node_labels : Ο τύπος κάθε κόμβου interaction, κωδικοποιημένος με το Label Encoder\n",
        "                             Dict{key = '_:p{i}', value = Label_Encoder(interaction type του i)\n",
        "        :param nodes_per_class : Οι κορυφές του γράφου οργανωμένες σύμφωνα με τον τύπο τους\n",
        "                Dict{key   = Ο τύπος/κλάση της κορυφής στο γράφο (Sentence,    Token,           Interaction),\n",
        "                     value = List[όλοι οι κόμβοι αυτού του τύπου (*.d{x}.s{y}, *.d{x}.s{y}.e{z}, _:p)\n",
        "        :return: Τους πίνακες χαρακτηριστικών των οντοτήτων, γειτνίασης και ετικετών των interation nodes\n",
        "\n",
        "        1. node_indexes :\n",
        "           Dict{ key   = Sentence, Token, Interaction\n",
        "                 value = Dict<key = οντότητα (όνομα), value = id οντότητας -> με την μορφή Α/Α}}\n",
        "        2. Για τις κορυφές Sentence και Token, δημιουργεί τα αρχικά διανύσματα χαρακτηριστικών\n",
        "           όπως προέκυψαν από το bert.\n",
        "           node_feature_vectors : Dict{ key = Sentence, Token\n",
        "                value = Tensor (float32) διάστασης πλήθος κορυφών x διάσταση διανύσματος χαρακτηριστικών}\n",
        "        3. Tensor διάστασης : πλήθος κόμβων interactions με τα labels τους\n",
        "        4. Δημιουργεί τους πίνακες γειτνίασης για κάθε τύπο ακμής.\n",
        "           Dict με key = τύπος ακμής ως τριπλέτα (Τύπος κορυφής subject, predicate, Τυπος κορυφής object)\n",
        "                   value Tensor (long) διάστασης 2 x πλήθος ακμών του συγκεκριμένου τύπου μέσα στο γράφημα.\n",
        "        \"\"\"\n",
        "        node_indexes = self.nodes_to_indexes(nodes_per_class)  # 1\n",
        "\n",
        "        # LOAD BERT EMBEDDINGS FILES\n",
        "        sentence_embeddings, token_embeddings = WordEmbeddings(train=self.train).word_embeddings()\n",
        "        emb_dim = next(iter(sentence_embeddings.values())).size(dim=0)\n",
        "\n",
        "        # CREATE NODE FEATURE VECTORS\n",
        "        node_feature_vectors = {}  # 2\n",
        "        for node_type, embeddings in [(SENT_CLASS, sentence_embeddings), (TOKEN_CLASS, token_embeddings)]:\n",
        "            node_feature_vectors[node_type] = self.create_node_feature_vectors(\n",
        "                nodes_per_class[node_type], embeddings, emb_dim\n",
        "            )\n",
        "\n",
        "        # CREATE INTERACTION NODE LABELS\n",
        "        node_labels = self.create_node_labels(nodes_per_class[INTERACTION_CLASS], node_labels,  # 3\n",
        "                                              node_indexes[INTERACTION_CLASS])\n",
        "\n",
        "        # CREATE EDGE INDEX MATRICES\n",
        "        edge_index = self.create_edge_index_matrices(subgraph, node_indexes)  # 4\n",
        "\n",
        "        return node_indexes, node_feature_vectors, edge_index, node_labels, emb_dim\n",
        "\n",
        "\n",
        "\n",
        "    def nodes_to_indexes(self, nodes_per_class):\n",
        "        \"\"\"\n",
        "        Αντιστοιχίζει κάθε οντότητα (κόμβο του γράφου) με έναν Α/Α που θα την αντιπροσωπεύει στον πίνακα\n",
        "        χαρακτηριστικών και γειτνίασης.\n",
        "        :param nodes_per_class: Οι κορυφές του γράφου οργανωμένες σύμφωνα με τον τύπο τους\n",
        "                Dict{key   = Ο τύπος/κλάση της κορυφής στο γράφο (Sentence,    Token,           Interaction),\n",
        "                     value = List[όλοι οι κόμβοι αυτού του τύπου (*.d{x}.s{y}, *.d{x}.s{y}.e{z}, _:p)\n",
        "        :return: node_indexes :\n",
        "           Dict{ key   = Sentence, Token, Interaction\n",
        "                 value = Dict<key = οντότητα (όνομα), value = id οντότητας -> με την μορφή Α/Α}}\n",
        "        \"\"\"\n",
        "        for node_type in nodes_per_class:\n",
        "            nodes_per_class[node_type] = \\\n",
        "                {node: i for i, node in enumerate(nodes_per_class[node_type])}\n",
        "        return nodes_per_class\n",
        "\n",
        "\n",
        "\n",
        "    def create_node_feature_vectors(self, nodes, embeddings, emb_dim):\n",
        "        \"\"\"\n",
        "        Δημιουργεί τον πίνακα των διανυσμάτων χαρακτηριστικών ενός τύπου κορυφής\n",
        "        του γραφήματος.\n",
        "        :param emb_dim: Διάσταση των word embeddings (base bert model = 768)\n",
        "        :param nodes :  List[όλοι οι κόμβοι αυτού του τύπου (Sentence : *.d{x}.s{y} ή Token : *.d{x}.s{y}.e{z})]\n",
        "        :param embeddings: Dict{key = ο identifier του κόμβου, όπως φαίνεται στο nodes,\n",
        "                                value = bert embedding αυτής της πρότασης ή του token}\n",
        "        :return: Tensor (float32) διάστασης πλήθος κορυφών x διάσταση διανύσματος χαρακτηριστικών\n",
        "                 Προκύπτει ως\n",
        "                 node_features[node_index] = word_embedding(οντότητα node_index)\n",
        "                 όπου node_index το id (Α/Α) της οντότητας\n",
        "        \"\"\"\n",
        "        # Πλήθος των κορυφών (οντοτήτων) του γράφου\n",
        "        n_nodes = len(nodes)\n",
        "\n",
        "        node_features = torch.empty((n_nodes, emb_dim), dtype=torch.float32)\n",
        "        for node, index in nodes.items():\n",
        "            node_features[index] = embeddings[node]\n",
        "\n",
        "        return node_features\n",
        "\n",
        "\n",
        "\n",
        "    def create_edge_index_matrices(self, subgraph, node_indexes):\n",
        "        \"\"\"\n",
        "        Δημιουργεί τους πίνακες με τις ακμές του γράφου.\n",
        "        :return:\n",
        "        - edge_index_matrices : Dict με key = τύπος ακμής ως τριπλέτα (Τύπος κορυφής subject, predicate, Τυπος κορυφής object)\n",
        "                    value Tensor (long) διάστασης 2 x πλήθος ακμών του συγκεκριμένου τύπου μέσα στο γράφημα.\n",
        "                    Οι ακμές αναπαριστάνεται με τη μορφή λίστας συντεταγμένων (COO format) άρα\n",
        "                    value[0] και value[1] : ids των αρχικών (οντότητα subject)\n",
        "                    και τελικών (οντότητα object) κορυφών αντίστοιχα\n",
        "        \"\"\"\n",
        "        edge_index_matrices = {}\n",
        "\n",
        "        for edge_type in subgraph[RDF_P].unique():\n",
        "            domain, range = get_domain_range(edge_type)\n",
        "            triples = subgraph[(subgraph[RDF_P] == edge_type)]\n",
        "\n",
        "            sub_index = [\n",
        "                node_indexes[domain][s] for s in triples[RDF_S].values]\n",
        "            obj_index = [\n",
        "                node_indexes[range][o] for o in triples[RDF_O].values]\n",
        "\n",
        "            edge_index_matrices[(domain, edge_type, range)] = torch.tensor([sub_index, obj_index], dtype=torch.long)\n",
        "\n",
        "        return edge_index_matrices\n",
        "\n",
        "\n",
        "\n",
        "    def add_inverse_edges(self):\n",
        "        for edge_type, edge_index in self.graph.edge_index_dict.items():\n",
        "            inv_predicate = get_inverse(edge_type[1])\n",
        "            if inv_predicate is None:\n",
        "                continue\n",
        "\n",
        "            inv_edge_index = torch.stack([edge_index[1], edge_index[0]], dim=0)\n",
        "            self.graph[(edge_type[2], inv_predicate, edge_type[0])].edge_index = inv_edge_index\n",
        "\n",
        "\n",
        "    def create_node_labels(self, interaction_nodes, labels, interaction_indexes):\n",
        "        \"\"\"\n",
        "        :param interaction_nodes: List[όλοι οι κόμβοι αυτού του τύπου (_:p)\n",
        "        :param labels: Ο τύπος κάθε κόμβου interaction, κωδικοποιημένος με το Label Encoder\n",
        "                Dict{key = '_:p{i}', value = Label_Encoder(interaction type του i)\n",
        "        :param interaction_indexes: Dict{'_:p : A/A που αντιστοιχεί στον κόμβο interaction}\n",
        "        :return: Tensor διάστασης : πλήθος κόμβων interactions με τα labels τους\n",
        "        \"\"\"\n",
        "        n_nodes = len(interaction_nodes)\n",
        "        labels_tensor = torch.empty(n_nodes, dtype=torch.long)\n",
        "        for node in interaction_nodes:\n",
        "            labels_tensor[interaction_indexes[node]] = torch.tensor(labels[node])\n",
        "        return labels_tensor\n",
        "\n",
        "# ----------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "    def train_val_split(self, val_percentage=0.3):\n",
        "        g_train = self.graph.clone()\n",
        "        transform = RandomNodeSplit(num_val=val_percentage, num_test=0, num_splits=1)\n",
        "        for store in g_train.node_stores:\n",
        "            if transform.key is not None and not hasattr(store, transform.key):\n",
        "                continue\n",
        "\n",
        "            train_masks, val_masks, test_masks = zip(\n",
        "                *[transform._split(store) for _ in range(transform.num_splits)])\n",
        "\n",
        "\n",
        "            store.train_mask = torch.stack(train_masks, dim=-1).squeeze(-1)\n",
        "            store.val_mask = torch.stack(val_masks, dim=-1).squeeze(-1)\n",
        "            store.test_mask = torch.stack(test_masks, dim=-1).squeeze(-1)\n",
        "\n",
        "        n_train_nodes = g_train[INTERACTION_CLASS].train_mask.count_nonzero().item()\n",
        "        return g_train, n_train_nodes\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def get_dataloader(g_train, sample_size=200, itr=4, batch_size=324):\n",
        "\n",
        "        g = torch.Generator()\n",
        "        g.manual_seed(0)\n",
        "\n",
        "        return HGTLoader(\n",
        "            g_train,\n",
        "            # Sample sample_size nodes per type and per iteration for itr iterations\n",
        "            num_samples={key: [sample_size] * itr for key in g_train.node_types},\n",
        "            # Use a batch size for sampling training nodes\n",
        "            batch_size=batch_size,\n",
        "            input_nodes=(INTERACTION_CLASS, g_train[INTERACTION_CLASS].train_mask),\n",
        "            shuffle=True,\n",
        "            drop_last=True,\n",
        "            generator=g\n",
        "        )\n",
        "\n",
        "    @staticmethod\n",
        "    def nodes_to_predict(graph, val_set = False):\n",
        "        if val_set:\n",
        "            indexes = graph[INTERACTION_CLASS].val_mask.nonzero(as_tuple=True)[0]\n",
        "        else:\n",
        "            indexes = graph[INTERACTION_CLASS].train_mask.nonzero(as_tuple=True)[0]\n",
        "        labels  = graph[INTERACTION_CLASS].y[indexes]\n",
        "        n_nodes = labels.size(0)\n",
        "        return indexes, labels, n_nodes\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def graph_stats(graph, val_set=False):\n",
        "        _, labels, _ = GraphDataset.nodes_to_predict(graph, val_set=val_set)\n",
        "        print(labels.unique(return_counts=True))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rx_fhEEzulOE",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title GNN Model\n",
        "\n",
        "from torch.nn.modules.container import ModuleDict\n",
        "\n",
        "\n",
        "class GNN_Model(nn.Module):\n",
        "\n",
        "    # hidden_channels -> final GNN layer node embedding (len(z_v))\n",
        "    def __init__(self, hidden_channels):\n",
        "\n",
        "        nn.Module.__init__(self)\n",
        "\n",
        "        # prediction head\n",
        "        self.W = Linear(in_channels= hidden_channels, out_channels= len(INTERACTION_CLASSES))\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def ConvLayer(lazy_init, conv_type, hidden_channels, act_func):\n",
        "\n",
        "        if conv_type == 'SAGEConv':\n",
        "            return SAGEConv(in_channels=lazy_init, out_channels=hidden_channels, normalize=True)\n",
        "        elif conv_type == 'GCNConv':\n",
        "            return GCNConv(in_channels=lazy_init, out_channels=hidden_channels, normalize=True)\n",
        "        elif conv_type == 'GINConv':\n",
        "            return GINConv(MLP(in_channels=-1, hidden_channels=hidden_channels, out_channels=hidden_channels,\n",
        "                               num_layers=3, act=act_func.__name__, dropout=0.4))\n",
        "        elif conv_type == 'GATConv':\n",
        "            return GATConv(in_channels=lazy_init, out_channels=hidden_channels, dropout=0.1)\n",
        "\n",
        "\n",
        "    def forward(self, data, nodes_to_predict):\n",
        "        x = data.x_dict\n",
        "        edge_index = data.edge_index_dict\n",
        "\n",
        "        z = self.encoder(x, edge_index)\n",
        "\n",
        "        y_hat = self.prediction_head(z, nodes_to_predict)\n",
        "\n",
        "        return y_hat\n",
        "\n",
        "\n",
        "\n",
        "    def prediction_head(self, z, nodes_to_predict):\n",
        "\n",
        "        y_hat = self.W(z[nodes_to_predict])\n",
        "\n",
        "        return y_hat if self.training else self.get_predictions(y_hat)\n",
        "\n",
        "\n",
        "    def get_predictions(self, y_hat):\n",
        "        y_hat = softmax(y_hat, dim=1)\n",
        "        return torch.argmax(y_hat, dim=1), y_hat\n",
        "\n",
        "# ---------------------------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "class GNN_with_PrePost(GNN_Model):\n",
        "\n",
        "    def __init__(self, in_channels, hidden_channels,\n",
        "                 preproc=None, postproc=None):\n",
        "\n",
        "        final_emb_dim = hidden_channels if postproc is None else postproc[-1]\n",
        "\n",
        "        GNN_Model.__init__(self, final_emb_dim)\n",
        "\n",
        "        self.pre_linear  = self.MLP_Net(in_channels,     preproc,  self.node_types)\n",
        "        self.post_linear = self.MLP_Net(hidden_channels, postproc, [INTERACTION_CLASS])\n",
        "\n",
        "\n",
        "    def encoder(self, x, edge_index):\n",
        "\n",
        "        if self.pre_linear is not None:\n",
        "            x = {node_type : self.pre_linear[node_type](x[node_type])\n",
        "                for node_type in self.pre_linear}\n",
        "            \n",
        "        x = self.conv_layers(x, edge_index)\n",
        "\n",
        "        if self.post_linear is not None:\n",
        "            x = {node_type : self.post_linear[node_type](x[node_type])\n",
        "                for node_type in self.post_linear}\n",
        "            \n",
        "        return x[INTERACTION_CLASS]\n",
        "\n",
        "\n",
        "    def MLP_Net(self, in_channels, hidden_layers, node_types):\n",
        "\n",
        "        if hidden_layers is None:\n",
        "            return None\n",
        "        mlp = ModuleDict({\n",
        "            node_type : MLP([in_channels] + hidden_layers, act = self.act_func.__name__, dropout=0.4)\n",
        "                for node_type in node_types\n",
        "        })\n",
        "        return mlp\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from torch_geometric.nn import BatchNorm\n",
        "\n",
        "\n",
        "class GNN(GNN_with_PrePost):\n",
        "\n",
        "    def __init__(self, in_channels, hidden_channels, \n",
        "                 node_types, edge_types,\n",
        "                 n_conv_layers, conv_type='SAGEConv', \n",
        "                 preproc=None, postproc=None, act_func=None\n",
        "    ):\n",
        "        self.node_types = node_types\n",
        "        self.n_conv_layers = n_conv_layers\n",
        "        self.act_func = act_func\n",
        "\n",
        "        GNN_with_PrePost.__init__(self, in_channels, hidden_channels,\n",
        "                                  preproc=preproc, postproc=postproc)\n",
        "        \n",
        "        # conv layers\n",
        "        self.convs = nn.ModuleList()\n",
        "\n",
        "        for _ in range(self.n_conv_layers):\n",
        "\n",
        "            conv = self.RConvLayer(conv_type, hidden_channels, edge_types)\n",
        "            self.convs.append(conv)\n",
        "\n",
        "        \"\"\"self.norm = nn.ModuleList([\n",
        "            BatchNorm(in_channels=hidden_channels)\n",
        "            for _ in range(self.n_conv_layers)\n",
        "        ])\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def RConvLayer(self, conv_type, hidden_channels, edge_types, aggr='sum'):\n",
        "\n",
        "        conv = {}\n",
        "        for edge_type in edge_types:\n",
        "            lazy_init = -1 if edge_type[0] == edge_type[2] else (-1, -1)\n",
        "            conv[edge_type] = GNN_Model.ConvLayer(lazy_init, conv_type, hidden_channels, self.act_func)\n",
        "\n",
        "        conv = HeteroConv(conv, aggr=aggr)\n",
        "        return conv  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def conv_layers(self, x, edge_index):\n",
        "\n",
        "             \n",
        "        for i in range(self.n_conv_layers):\n",
        "            x = self.convs[i](x, edge_index)\n",
        "            #x[INTERACTION_CLASS] = self.norm[i](x[INTERACTION_CLASS])\n",
        "\n",
        "            x = {node_type : dropout(x_i, p=0.4, training=self.training) for node_type, x_i in x.items()}\n",
        "\n",
        "            if i < self.n_conv_layers - 1:\n",
        "                x = self.act(x)\n",
        "                \n",
        "            return x\n",
        "\n",
        "\n",
        "    def act(self, x):\n",
        "        return {node_type : self.act_func(x_i) for node_type, x_i in x.items()} \\\n",
        "        if self.act_func is not None else x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "h4oJUU2MvJu9"
      },
      "outputs": [],
      "source": [
        "#@title Set Model\n",
        "\n",
        "\n",
        "def ClassificationModel(load_model=False):\n",
        "    with open(MODEL_CONFIG_FILE, \"r\") as fp:\n",
        "        model_config = json.load(fp)\n",
        "\n",
        "    model_config['act_func'] = _resolve_act_func(model_config['act_func'])\n",
        "\n",
        "    model = GNN(\n",
        "        # Πάντα ίδια :\n",
        "        in_channels=model_config['in_channels'],\n",
        "        hidden_channels=model_config['hidden_channels'],\n",
        "        node_types=model_config['node_types'],\n",
        "        edge_types=[tuple(edge) for edge in model_config['edge_types']],\n",
        "\n",
        "        # Παράμετροι μοντέλου :\n",
        "        preproc=model_config['preproc'],\n",
        "        postproc=model_config['postproc'],\n",
        "        n_conv_layers=model_config['n_conv_layers'],\n",
        "        conv_type=model_config['conv_type'],\n",
        "\n",
        "        act_func=model_config['act_func'],\n",
        "\n",
        "    )\n",
        "    if load_model:\n",
        "        if not files_found():\n",
        "            raise Exception('Saved model file not found. Train the model')\n",
        "\n",
        "        model.load_state_dict(torch.load(MODEL_FILE))\n",
        "        model.eval()\n",
        "    model.to(device())\n",
        "    print(model)\n",
        "    return model\n",
        "\n",
        "\n",
        "def _resolve_act_func(act_func):\n",
        "    if act_func == 'tanh':\n",
        "        return tanh\n",
        "    elif act_func == 'leaky_relu':\n",
        "        return leaky_relu\n",
        "    elif act_func == 'relu':\n",
        "        return relu\n",
        "\n",
        "\n",
        "def files_found():\n",
        "    return exists(MODEL_FILE) and exists(MODEL_CONFIG_FILE)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Train Model\n",
        "\n",
        "TRAINING = 1\n",
        "INFERENCE = 2\n",
        "\n",
        "# ================================================================================================\n",
        "\n",
        "\n",
        "class TrainClassificationModel:\n",
        "\n",
        "    def __init__(self, mode=TRAINING):\n",
        "\n",
        "        set_seed()\n",
        "        self.device = device()\n",
        "        self.whole_set = True if VAL_PREC == 0 else False\n",
        "\n",
        "        if not files_found():\n",
        "            mode = TRAINING\n",
        "\n",
        "        if mode == TRAINING:\n",
        "\n",
        "            self.dataset, self.g_train, self.n_interaction_nodes, \\\n",
        "                self.loader, self.g_test = self.prepair_dataset()\n",
        "\n",
        "            self.save_model_config()\n",
        "            self.model = ClassificationModel()\n",
        "\n",
        "            self.criterion = nn.CrossEntropyLoss()\n",
        "            self.optimizer = torch.optim.Adam(params=self.model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "            self.train_model()\n",
        "            torch.save(self.model.state_dict(), MODEL_FILE)\n",
        "\n",
        "            self.evaluate_test_set()\n",
        "        else:\n",
        "            self.inference()\n",
        "\n",
        "\n",
        "    def inference(self):\n",
        "        \"\"\"\n",
        "        Φορτώνει το εκπαιδευμένο μοντέλο και το εφαρμόζει στο σύνολο ελέγχου\n",
        "        \"\"\"\n",
        "        self.model = ClassificationModel(load_model=True)\n",
        "        self.evaluate_test_set()\n",
        "\n",
        "\n",
        "\n",
        "    def prepair_dataset(self):\n",
        "        \"\"\"\n",
        "        Δημιουργεί ή φορτώνει αν υπάρχουν αποθηκευμένα τα σύνολα εκπαίδευσης και ελέγχου\n",
        "        :return:\n",
        "            dataset: Αντικείμενο Graph_Dataset για το σύνολο εκπαίδευσης\n",
        "            g_train: HeteroData ο γράφος του συνόλου εκπαίδευσης\n",
        "            n_train_nodes: Το πλήθος των κόμβων Interaction που χρησιμοποιούνται για εκπαίδευση\n",
        "            loader: HGTLoader Dataloader για minibatch training\n",
        "            g_test: HeteroData ο γράφος του συνόλου ελέγχου\n",
        "        \"\"\"\n",
        "\n",
        "        dataset = GraphDataset(train=True)\n",
        "        g_train, n_train_nodes = (dataset.graph, dataset.n_interaction_nodes) if self.whole_set else \\\n",
        "                                  dataset.train_val_split(val_percentage=VAL_PREC)\n",
        "        g_train.to(self.device)\n",
        "        GraphDataset.graph_stats(g_train)\n",
        "        loader = dataset.get_dataloader(g_train)\n",
        "\n",
        "        g_test = GraphDataset(train=False).graph\n",
        "        g_test.to(self.device)\n",
        "\n",
        "        return dataset, g_train, n_train_nodes, loader, g_test\n",
        "\n",
        "\n",
        "\n",
        "    def save_model_config(self):\n",
        "        \"\"\"\n",
        "        Δημιουργεί το αρχείο classification_model_config.json με τις παράμετρους\n",
        "        του μοντέλου όπως έχουν οριστεί στο config.py\n",
        "        \"\"\"\n",
        "        model_config = {\n",
        "            'in_channels': self.dataset.emb_dim,\n",
        "            'node_types': self.dataset.graph.metadata()[0],\n",
        "            'edge_types': self.dataset.graph.metadata()[1],\n",
        "\n",
        "            'preproc': MLP_PREPROCESSING_DIM,\n",
        "            'postproc': MLP_POSTPROCESSING_DIM,\n",
        "            'n_conv_layers': 1,\n",
        "            'hidden_channels': HIDDEN_CHANNELS,\n",
        "            'conv_type': GNN_TYPE,\n",
        "\n",
        "            'act_func': ACTIVATION_FUNC\n",
        "        }\n",
        "        with open(MODEL_CONFIG_FILE, \"w\") as fp:\n",
        "            json.dump(model_config, fp)\n",
        "\n",
        "# ================================================================================================\n",
        "\n",
        "    def train_model(self):\n",
        "        \"\"\"\n",
        "        Εκτελεί την εκπαίδευση του μοντέλου και τυπώνει τα αποτελέσματα για κάθε σετ\n",
        "        ανά 10 εποχές.\n",
        "        \"\"\"\n",
        "        with torch.no_grad():  # lazy init\n",
        "            nodes, _, _ = self.get_labels(self.g_train)\n",
        "            self.model(data=self.g_train, nodes_to_predict=nodes)\n",
        "\n",
        "        for epoch in range(EPOCHS):\n",
        "            # mini-batch\n",
        "            running_loss = sum([self.train(batch) for batch in self.loader]) / self.n_interaction_nodes\n",
        "\n",
        "            if epoch % 10 == 0:\n",
        "                results = {}\n",
        "                results['TRAIN'] = self.evaluate(self.g_train)\n",
        "                if not self.whole_set:\n",
        "                    results['VAL'] = self.evaluate(self.g_train, val_set=True)\n",
        "                results['TEST'] = self.evaluate(self.g_test, show_confmat=True, get_per_class=True)\n",
        "                self.print_results(epoch, running_loss, results)\n",
        "\n",
        "\n",
        "# ================================================================================================\n",
        "\n",
        "    def get_labels(self, data, val_set=False):\n",
        "        \"\"\"\n",
        "        :param data: Αντικείμενο HeteroData. Μπορεί να είναι ένα batch του loader\n",
        "                     ή ολόκληρο σύνολο\n",
        "        :param val_set: Αν πρόκειται για το σύνολο επικύρωσης True (default=False)\n",
        "        :return: nodes_to_predict (tensor) : Τα indexes των κόμβων Interaction που θα προβλεφθεί\n",
        "                 η κλάση τους από το μοντέλο\n",
        "                 labels (tensor) : Η πραγματική κλάση κάθε τέτοιου κόμβου\n",
        "                 n_nodes (int) : Το πλήθος τους\n",
        "        \"\"\"\n",
        "        nodes_to_predict, labels, n_nodes = GraphDataset.nodes_to_predict(data, val_set=val_set)\n",
        "\n",
        "        return nodes_to_predict, labels, n_nodes\n",
        "\n",
        "# ================================================================================================\n",
        "\n",
        "    def train(self, batch):\n",
        "        \"\"\"\n",
        "        Εκτελεί μία εποχή εκπαίδευσης για ένα batch\n",
        "        :param batch: HeteroData Ένα κομμάτι του γράφου από τον loader\n",
        "        :return: Το running loss της εποχής στο συγκεκριμένο batch\n",
        "        \"\"\"\n",
        "        self.model.train()\n",
        "        self.optimizer.zero_grad()\n",
        "        # batch = self.drop_edges(batch)\n",
        "\n",
        "        nodes_to_predict, batch_labels, n_nodes = self.get_labels(batch)\n",
        "        y_hat = self.model(data=batch, nodes_to_predict=nodes_to_predict)\n",
        "\n",
        "        loss = self.criterion(y_hat, batch_labels)\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss.item() * n_nodes\n",
        "\n",
        "\n",
        "    def drop_edges(self, batch):\n",
        "        \"\"\"\n",
        "        Αφαιρεί κάποιες ακμές από το γράφο batch\n",
        "        :param batch: HeteroData Ένα κομμάτι του γράφου από τον loader\n",
        "        :return: Το ίδιο αντικείμενο με p % τυχαία απορριφθέντες ακμές\n",
        "                  για κάθε τύπο ακμής\n",
        "        \"\"\"\n",
        "        for edge_type in batch.metadata()[1]:\n",
        "            edge_index = batch[edge_type].edge_index\n",
        "            edge_index, _ = dropout_adj(edge_index, p=0.4, training=True)\n",
        "            batch[edge_type].edge_index = edge_index\n",
        "        return batch\n",
        "\n",
        "# ============================================================================================================\n",
        "\n",
        "    def evaluate_test_set(self):\n",
        "        \"\"\"\n",
        "        Εφαρμόζει το εκπαιδευμένο μοντέλο για το σύνολο ελέγχου κατά\n",
        "        την λειτουργία INFERENCE.\n",
        "        \"\"\"\n",
        "        self.dataset = GraphDataset(train=False)\n",
        "        g_test = self.dataset.graph\n",
        "        g_test.to(self.device)\n",
        "        results, y_hat, labels = self.evaluate(g_test, show_confmat=True, get_per_class=True, error_an=True)\n",
        "        self.print_results('INFERENCE', 0., {'TEST': results})\n",
        "        self.incorrect_pairs(self.dataset.node_indexes[INTERACTION_CLASS], y_hat, labels)\n",
        "\n",
        "\n",
        "    def incorrect_pairs(self, pairs, y_hat, labels):\n",
        "        \"\"\"\n",
        "        Εντοπίζει τα λάθη που έγιναν από το μοντέλο. Αν στο config.py\n",
        "        οριστεί PRINT_MISCLS=True τότε θα παράγει λεπτομερή αναφορά\n",
        "        των σφαλμάτων.\n",
        "        :param pairs: Dict{interaction_node_id : index στο GraphDataset}\n",
        "        :param y_hat: Οι προβλέψεις του μοντέλου\n",
        "        :param labels: Οι πραγματική κλάση κάθε κόμβου interaction\n",
        "        \"\"\"\n",
        "        pairs = {index: pair for pair, index in pairs.items()}\n",
        "        errors = []\n",
        "\n",
        "        for i, (cl, pred) in enumerate(zip(labels, y_hat)):\n",
        "            if cl == pred: continue\n",
        "            errors.append({INTERACTION_CLASS: pairs[i], 'y': cl.item(), 'y_hat': pred.item()})\n",
        "\n",
        "        errors = pd.DataFrame(errors)\n",
        "        for col in ['y', 'y_hat']:\n",
        "            errors[col] = self.dataset.label_enc.inverse_transform(errors[col])\n",
        "        print(' test set size =', labels.size(0))\n",
        "        print('misclassified  =', errors.shape[0])\n",
        "        errors.to_csv('misclassified.csv', sep=',', encoding='utf-8', index=False)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def evaluate(self, data, show_confmat=False, get_per_class=False, error_an=False, val_set=False):\n",
        "        \"\"\"\n",
        "        :param data: HeteroData ένα σύνολο γράφου\n",
        "        :param show_confmat: True αν θα εμφανίσει πίνακα σύγχυσης (default False)\n",
        "        :param get_per_class: True αν θα δώσει αποτελέσματα για κάθε κλάση ξεχωριστά (default False)\n",
        "        :param error_an: True αν θα επιστρέψει επιπλέον τις προβλέψεις του μοντέλου και τα labels (default False)\n",
        "        :param val_set: True αν επιθυμείται εκτίμηση στο validation set (default False)\n",
        "        :return: results : 2D Dict Για κάθε μετρική επιστρέφει το αποτέλεσμα\n",
        "                Αν error_an == True επιστρέφει επιπλέον:\n",
        "                 y_hat : Οι προβλέψεις του μοντέλου\n",
        "                 eval_labels : Οι πραγματικές κλάσεις\n",
        "        \"\"\"\n",
        "        # forward pass\n",
        "        with torch.no_grad():\n",
        "            self.model.eval()\n",
        "            nodes_to_predict, eval_labels, n_nodes = self.get_labels(data, val_set=val_set)\n",
        "            y_hat, _ = self.model(data=data, nodes_to_predict=nodes_to_predict)\n",
        "\n",
        "        y_hat_detection = self.turn_to_binary(y_hat)\n",
        "        labels_detection = self.turn_to_binary(eval_labels)\n",
        "\n",
        "        # ------------------------------------------------------------\n",
        "        # METRICS\n",
        "        # ------------------------------------------------------------\n",
        "        precision_metric = torchmetrics.functional.classification.precision\n",
        "        recall_metric    = torchmetrics.functional.classification.recall\n",
        "        f1_metric        = torchmetrics.functional.classification.f_beta.f1_score\n",
        "\n",
        "        if show_confmat:\n",
        "            self.conf_matrix(y_hat, eval_labels)\n",
        "\n",
        "        results = {}\n",
        "\n",
        "        for metric in [f1_metric, precision_metric, recall_metric]:\n",
        "\n",
        "            metric_name = metric.__name__\n",
        "            results[metric_name] = {}\n",
        "            results[metric_name]['micro'] = metric(y_hat, eval_labels).item()\n",
        "            results[metric_name]['micro+'] = metric(y_hat, eval_labels, ignore_index=self.dataset.negative_label).item()\n",
        "            results[metric_name]['binary'] = metric(y_hat_detection, labels_detection).item()\n",
        "\n",
        "            if get_per_class:\n",
        "                metric_per_class = metric(y_hat, eval_labels, average=None,\n",
        "                                          num_classes=len(INTERACTION_CLASSES))\n",
        "\n",
        "                for class_, value in enumerate(metric_per_class):\n",
        "                    results[metric_name][self.dataset.label_enc.inverse_transform([class_])[0]] = value.item()\n",
        "\n",
        "        return results if not error_an else (results, y_hat, eval_labels)\n",
        "\n",
        "\n",
        "    def turn_to_binary(self, labels):\n",
        "        \"\"\"\n",
        "        Μετατρέπει τις τιμές του labels σε δυαδικές.\n",
        "        Για κάθε l στο labels αν είναι η αρνητική κλάση True, για τις άλλες False\n",
        "        \"\"\"\n",
        "        return torch.tensor([l == self.dataset.negative_label for l in labels], dtype=torch.bool)\n",
        "\n",
        "\n",
        "    def conf_matrix(self, y_hat, labels):\n",
        "        \"\"\"\n",
        "        Δημιουργεί και εμφανίζει τον πίνακα σύγχυσης.\n",
        "        \"\"\"\n",
        "        confmat = ConfusionMatrix(num_classes=len(self.dataset.label_enc.classes_))\n",
        "        confmat.to(self.device)\n",
        "        cm = confmat(y_hat, labels).cpu().detach().numpy()\n",
        "\n",
        "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=self.dataset.label_enc.classes_)\n",
        "        disp.plot()\n",
        "        plt.show()\n",
        "\n",
        "# =========================================================================================================\n",
        "    def print_results(self, epoch, running_loss, results):\n",
        "\n",
        "        print(f'> Epoch {epoch}\\n\\tLoss = {\"{:.5f}\".format(running_loss)}')\n",
        "        for set_, values in results.items():\n",
        "            values = pd.DataFrame(values).T\n",
        "            print(f'{set_}\\n{values}')\n",
        "\n",
        "        print('_' * 100)\n",
        "# =========================================================================================================\n",
        "\n"
      ],
      "metadata": {
        "id": "PrZnVql6gRp7",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Ρυθμός μάθησης\n",
        "LR = 0.00005\n",
        "\n",
        "# Weight decay για το Adam optimizer\n",
        "WEIGHT_DECAY = 5 * 1e-4\n",
        "\n",
        "# Πλήθος εποχών εκπαίδευσης\n",
        "EPOCHS = 171\n",
        "\n",
        "# Μέγεθος συνόλου επικύρωσης. Πρέπει να είναι μεταξύ 0 και 1\n",
        "# (πχ 0.3 -> 30% του αρχικού συνόλου εκπαίδευσης θα χρησιμοποιηθεί ως επικύρωσης).\n",
        "# Αν 0 τότε ολόκληρο το σύνολο εκπαίδευσης θα χρησιμοποιηθεί για εκπαίδευση\n",
        "VAL_PREC = 0\n",
        "\n",
        "# Για evaluation ενός αποθηκευμένου μοντέλου στο test set -> 'INFERENCE'\n",
        "# (αν δε βρεθεί αποθηκευμένο μοντέλο η εκπαίδευση θα εκτελεστεί και θα αποθηκευτεί το μοντέλο)\n",
        "# Ή για εκπαίδευση μοντέλου -> 'TRAINING'\n",
        "MODE = 'TRAINING'\n",
        "\n",
        "# Πλήθος νευρώνων σε κάθε επίπεδο προεπεξεργασίας\n",
        "# πχ [256] ένα επίπεδο προεπεξεργασίας με 256 νευρώνες\n",
        "#    None : κανένα επίπεδο προεπεξεργασίας\n",
        "MLP_PREPROCESSING_DIM = None #[256]\n",
        "\n",
        "# Όμοια για μετεπεξεργασίας\n",
        "MLP_POSTPROCESSING_DIM = [128, 64]\n",
        "\n",
        "# Τελεστής GNN που θα εφαρμοστεί πχ 'SAGEConv', 'GINConv', 'GATConv'\n",
        "GNN_TYPE = 'GINConv'\n",
        "\n",
        "# Διάσταση διανυσματικού χώρου του GNN\n",
        "HIDDEN_CHANNELS = 256\n",
        "\n",
        "# Συνάρτηση ενεργοποίησης πχ 'relu', 'leaky_relu', 'tanh'\n",
        "ACTIVATION_FUNC = 'relu'\n",
        "\n"
      ],
      "metadata": {
        "id": "FofZWRRNYErN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPrmxqZmhpu8"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Training\n",
        "TrainClassificationModel(mode = INFERENCE if MODE=='INFERENCE' else TRAINING)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br><br>\n",
        "Res:\n",
        "<br><br>"
      ],
      "metadata": {
        "id": "87a1AqCOJkzM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "TrainClassificationModel(mode = INFERENCE)\n"
      ],
      "metadata": {
        "id": "N2iUhJpAY8kz"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}